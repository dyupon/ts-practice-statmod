\section{Exponential smoothing, модели тренда, ES и ARIMA}

Аббревиатура ETS(M,A,N) расшифровывается как Error, Trend, Seasonal. Далее мы расскажем, что модель может быть применена для ряда случаев: для аддитивных и мультипликативных ошибок, постоянного или монотонного тренда, мультипликативной или аддитивной сезонности. Каждый из случаев носит название, состоящее из трех букв. Например ETS(M, A, N) будет означать, что у ряда мультипликативные ошибки и монотонный тренд без сезонных колебаний.

\subsection{Простое экспоненциальное сглаживане}
Ранее мы рассматривали скользящее среднее. Мы можем использовать его для построения простейшего предсказания: $\hat{x}_{N+1}=\frac{1}{N}\sum_{i=1}^{N}x_i$, где $N$ --- длина исходного ряда. Заметим, что в таком определении все точки получают одинаковый вес, одинаково влияют на предсказание. Нам бы хотелось, чтобы влияние старых наблюдений было меньше.

Давайте сделаем такой фильтр. Выберем некоторое $0<\alpha<1$ и положим 
\[\hat{x}_{N+1}=\alpha x_N +\alpha(1 - \alpha)x_{N - 1} + \alpha(1 - \alpha)^2x_{N - 2} + \ldots \] 
При таком определении у старых наблюдений вес убывает экспоненциально.
Перепишем нашу формулу:
\[ \hat{x}_{N+1}=\alpha x_N +(1 - \alpha )l_n.  \]
Аналогично мы определим и предыдущие элементы ряда:
\[ 2 \leq n \leq N, l_n =\alpha x_{n-1} + (1 - \alpha)l_{n-1}. \]

На самом деле мы определили бесконечный линейный фильтр (Infinite Response Filter), но мы имеем дело с конечными последовательностями, поэтому нам необходимо найти начальное значение, дальше которого формулу мы не разворачиваем: $l_2=\alpha x_1+(1 - \alpha )l_1$. $l_1$ – это начальное значение для нашего фильтра, и мы находим его с помощью метода наименьших квадратов вместе с параметром $\alpha$. Теперь мы можем явно выразить предсказание:
\[\hat{x}_{N+1} = \sum_{i=1}^{N} \alpha (1 - \alpha )^{N - i} x_i + (1 - \alpha )l_1.\]

Для дальнейших рассуждений нам удобно представить формулу в составном виде:
\begin{align*}
& \hat{x}_{N+t} l_{n+1} = l_n+1, t \geq 1, \\
& l_{n+1} = \alpha x_n +(1 - \alpha )l_n ,1 \leq n \leq N.
\end{align*}
\begin{itemize}
\item $x_n$  – данные, с которыми мы работаем, исходный временной ряд.
\item $l_n$  – предсказанное для $x_n$  значение с помощью сглаживания. 
\end{itemize}

С помощью такой формулы мы сможем предсказывать значение только для рядов без тренда и сезонности: мы предсказываем одно и то же будущее значение и никак не учитываем изменения ряда.

\subsection{Выделение линейного тренда}
Хольтом (Holt) в 1957 было предложено расширение метода для рядов с линейным трендом:

\begin{align*}
\hat x_{N+t} &= l_{N+1} + t \cdot b_{N+1},\ t \geq 1 \\ l_{n+1} &= \alpha x_n + (1 - \alpha)(l_n - b_n), 1 \leq n \leq N \\ b_{n+1} &= \beta^*(l_{n+1} - l_n) + (1 - \beta^*) b_n, 1 \leq n \leq N 
\end{align*}

$b_{N+1}$ определяет скорость роста ряда, также как и для $l_n$ ,нам понадобится найти начальное значение параметра $b_1$ и $\beta^*$ с помощью метода наименьших квадратов.

\subsection{Замедление скорости роста}
Описанная ранее формула предполагает неограниченный рост значений ряда для t??. Мы хотели бы предсказывать не столь большие значения для далеких предсказаний. Для этого мы немного изменим формулу:

\begin{align*} 
\hat x_{N+t} &= l_{N+t} + (\sum_{i=1}^{t}{\phi^t}) b_{N+1} \\ l_{n+1} &= \alpha x_n + (1 - \alpha)(l_n - \phi b_n), 1 \leq n \leq N \\ b_{n+1} &= \beta^*(l_{n+1} - l_n) + (1 - \beta^*) \phi b_n, 1 \leq n \leq N 
\end{align*}

Полагая $0 < \phi < 1$, получим что сумма $\sum_{i=1}^{t}{\phi^t}$ оказывается конечной для $t \to \infty$ и равна $\frac{1}{1 - \phi}$. Таким образом мы можем ограничить значения для далеких предсказаний.

\subsection{Holt-Winters сезонное сглаживание}
Винтерс (Winters) в 1960 расширил метод, предложенный Хольтом, добавив сезонную составляющую. Обозначим $m$ – частоту сезонности, $m \geq 1$. Например, для ежемесячных наблюдений $m=12$, для квартальных --- $m=4$.

\begin{align*}
\hat x_{N+t} &= l_{N+1} + t \cdot b_{N+1} + s_{N + t - m (\lfloor \frac{t-1}{m} \rfloor + 1)},\ t \geq 1 \\ l_{n+1} &= \alpha (x_n - s_{n - m + 1}) + (1 - \alpha)(l_n - b_n), 1 \leq n \leq N \\ b_{n+1} &= \beta^*(l_{n+1} - l_n) + (1 - \beta^*) b_n, 1 \leq n \leq N  \\ s_{n+1} &= \gamma(x_{n+1} - l_n - b_n) + (1 - \gamma)s_{n - m + 1} 
\end{align*}

Добавилось ещё $m + 1$ параметров: $s_1, \ldots, s_m$ – начальные значения для сезонностей и параметр $\gamma$, которые нам нужно определить как и раньше с помощью МНК. Мы также можем замедлить скорость роста тренда при $t \to \infty$, заменив $t \cdot b_{N+1}$ на $\sum_{i=1}^{t}{\phi^t} b_{N+1}$.

Также метод может быть обобщен для мультипликативных рядов.

\subsection{State Space Model}
До сих пор мы рассматривали методы для точечных предсказаний, но мы хотим также строить доверительные интервалы для них, а также уметь сравнивать разные методы. Нам понадобится ввести статистическую модель для этого.

Для начала рассмотрим простое экспоненциальное сглаживание:
\begin{align*}
\hat x_{N+1} &= l_{N+1} \\ l_{n+1} &= \alpha x_n + (1 - \alpha) l_n.
\end{align*}

Мы можем переписать второе уравнение в следующем виде:
\[ l_{n+1} = l_n + \alpha(x_n - l_n). \]

Заметим, что второе слагаемое в равной части соответствует ошибке предсказания, давайте обозначим её за $\varepsilon_n$. Тогда $l_{n+1} = l_n + \alpha \varepsilon_n$. Предположим, что $\varepsilon_n \sim N(0, \sigma^2)$, независимы и одинаково распределенные.

Мы получим следующие уравнения:

\begin{align*} 
x_n &= l_n + \varepsilon_n, \\ 
l_{n+1} &= l_n + \alpha\varepsilon_n.
\end{align*}

Первое позволяет установить связь с элементам исходного ряда, а последнее задаёт правило перехода к предсказанию следующего наблюдения. Заметим, что если $\alpha=1$, то мы получим простое случайное блуждание.

Для модели с линейным трендом мы обозначим $\varepsilon_n = x_n  - l_n  - b_n$ , тогда мы можем переписать наши формулы в следующем виде:

\begin{align*} 
x_n &= l_n + b_n + \varepsilon_n, \\ 
l_{n+1} &= l_n + b_n + \alpha\varepsilon_n, \\ 
b_{n+1} &= b_n + \beta \varepsilon_n, 
\end{align*}
где $\beta =\alpha \beta^*$. Для ряда с сезонностью с периодом $m$ и линейным трендом, предполагая $\varepsilon_n = x_n - l_n - b_n - s_{n - m}$, мы получим следующие формулы:

\begin{align*} 
x_n &= l_n + b_n + s_{n - m + 1} + \varepsilon_n, \\ 
l_{n+1} &= l_n + b_n + \alpha\varepsilon_n, \\ 
b_{n+1} &= b_n + \beta \varepsilon_n, \\ 
s_{n+1} &= s_{n-m+1} + \gamma \varepsilon_n.
\end{align*}

Мы также можем определить модели для случая мультипликативной сезонности, мультипликативных ошибок, а также модель с подавлением роста тренда.

\subsection{Выбор модели}
Теперь, когда мы определили модель данных, мы можем находить параметры с помощью метода максимального правдоподобия. Кроме того, с помощью метода максимального правдоподобия мы можем сравнивать разные модели.

Определим информационные критерии на основе значения функции правдоподобия:

\begin{align*} 
\text{AIC} &= -2\log(L) + 2k, \\ 
\text{AIC}_c &= \text{AIC} + \frac{k(k+1)}{N - k -1}, \\ 
\text{BIC} &= \text{AIC} + k[\log(N) - 2],
\end{align*}
где $L$ --- значение функции правдоподобия, $k$ --- количество параметров модели. Такие критерии позволяют учитывать сложность модели и количество наблюдений.

Существует эквивалентность между ARIMA и некоторыми моделями экспоненциального сглаживания. Например, модель с постоянным трендом и без сезонности (Simple Exponential smoothing) эквивалентна модели ARIMA(0,1,1). Модель с линейным трендом и без сезонности эквивалентна ARIMA(0,2,2).